{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.optimizers import adam\n",
    "\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from DQN import Agent\n",
    "\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 84\n",
    "img_width = 84\n",
    "img_channels = 3\n",
    "\n",
    "state_dim = [img_height,img_width,img_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(observation):\n",
    "    observation = cv2.cvtColor(cv2.resize(observation, (84, 110)), cv2.COLOR_BGR2GRAY)\n",
    "    observation = observation[26:110,:]\n",
    "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation,(84,84,1))/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateQValues(Q,reward_total):\n",
    "    \n",
    "    Qret = Q\n",
    "    \n",
    "    Q[0][1] = [0] * (nb_actions)\n",
    "    Q[1][1] = [0] * (nb_actions)\n",
    "    \n",
    "    for i in range(2,len(Q)):\n",
    "        \n",
    "        state = np.dstack([Q[i-2][0], Q[i-1][0], Q[i][0]])\n",
    "    \n",
    "        state_pred = np.expand_dims(state, axis=0)\n",
    "        actions_pred = DQN_Agent.model.predict(state_pred)[0]\n",
    "        \n",
    "        action_values = [0] * (nb_actions)\n",
    "        action_values[Q[i][1]] = reward_total*(0.98)**(len(Q)-i) #here we nedd Q0(S,A)\n",
    "        \n",
    "        actions_values = []\n",
    "\n",
    "        for num1,num2 in zip(actions_pred,action_values):\n",
    "            actions_values.append(num1+num2)\n",
    "        Qret[i][1] = actions_values\n",
    "    \n",
    "    \n",
    "    return Qret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e0150af54795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "Qi0][1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "3.0\n",
      "1.0\n",
      "4.0\n",
      "4.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "Epoch 1/1\n",
      "4296/4296 [==============================] - 18s 4ms/step - loss: 3083568635370.0708 - mean_absolute_error: 938186.8489\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "0.0\n",
      "3.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "Epoch 1/1\n",
      "4286/4286 [==============================] - 19s 4ms/step - loss: 3568725014075.9683 - mean_absolute_error: 1009289.7972\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "Epoch 1/1\n",
      "3636/3636 [==============================] - 15s 4ms/step - loss: 2721293568944.0176 - mean_absolute_error: 894093.8063\n",
      "3.0\n",
      "0.0\n",
      "4.0\n",
      "0.0\n",
      "2.0\n",
      "3.0\n",
      "3.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "4.0\n",
      "0.0\n",
      "1.0\n",
      "Epoch 1/1\n",
      "4020/4020 [==============================] - 17s 4ms/step - loss: 3316310738069.7793 - mean_absolute_error: 976647.0572\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "0.0\n",
      "4.0\n",
      "1.0\n",
      "2.0\n",
      "7.0\n",
      "0.0\n",
      "Epoch 1/1\n",
      "4292/4292 [==============================] - 18s 4ms/step - loss: 3747095701357.0327 - mean_absolute_error: 1040862.3790\n",
      "4.0\n",
      "3.0\n",
      "3.0\n",
      "2.0\n",
      "9.0\n",
      "3.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "3.0\n",
      "6.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "Epoch 1/1\n",
      "4733/4733 [==============================] - 20s 4ms/step - loss: 4137195020597.8179 - mean_absolute_error: 1086943.3088\n",
      "3.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "4.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "4.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "4.0\n",
      "2.0\n",
      "3.0\n",
      "Epoch 1/1\n",
      "4264/4264 [==============================] - 18s 4ms/step - loss: 3677589800510.4390 - mean_absolute_error: 1027637.2195\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "3.0\n",
      "4.0\n",
      "Epoch 1/1\n",
      "4009/4009 [==============================] - 17s 4ms/step - loss: 2996394554750.8823 - mean_absolute_error: 910635.7484\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "5.0\n",
      "2.0\n",
      "Epoch 1/1\n",
      "3605/3605 [==============================] - 15s 4ms/step - loss: 2831430770644.8247 - mean_absolute_error: 889826.2967\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "3.0\n",
      "2.0\n",
      "0.0\n",
      "4.0\n",
      "2.0\n",
      "Epoch 1/1\n",
      "4239/4239 [==============================] - 18s 4ms/step - loss: 3556627362962.1475 - mean_absolute_error: 1010640.4727\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "3.0\n",
      "3.0\n",
      "4.0\n",
      "4.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "4.0\n",
      "0.0\n",
      "3.0\n",
      "Epoch 1/1\n",
      "4529/4529 [==============================] - 19s 4ms/step - loss: 3606559272552.1182 - mean_absolute_error: 996896.9517\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "0.0\n",
      "4.0\n",
      "3.0\n",
      "2.0\n",
      "2.0\n",
      "Epoch 1/1\n",
      "3961/3961 [==============================] - 16s 4ms/step - loss: 3097776415814.0591 - mean_absolute_error: 946209.5452\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "4.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "4.0\n",
      "0.0\n",
      "1.0\n",
      "Epoch 1/1\n",
      "4166/4166 [==============================] - 17s 4ms/step - loss: 3432421110167.0435 - mean_absolute_error: 972278.7638\n",
      "4.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "Epoch 1/1\n",
      "3402/3402 [==============================] - 14s 4ms/step - loss: 2207137283739.6167 - mean_absolute_error: 780037.6155\n",
      "2.0\n",
      "1.0\n",
      "4.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "4.0\n",
      "3.0\n",
      "Epoch 1/1\n",
      "3979/3979 [==============================] - 17s 4ms/step - loss: 2423692041945.5903 - mean_absolute_error: 831863.2030\n",
      "2.0\n",
      "2.0\n",
      "9.0\n",
      "3.0\n",
      "0.0\n",
      "1.0\n",
      "4.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "Epoch 1/1\n",
      "4506/4506 [==============================] - 19s 4ms/step - loss: 2618152631629.1523 - mean_absolute_error: 863135.8538\n",
      "2.0\n",
      "4.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "4.0\n",
      "3.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "4.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "Epoch 1/1\n",
      "4542/4542 [==============================] - 19s 4ms/step - loss: 2348842443945.9902 - mean_absolute_error: 819617.1507\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "5.0\n",
      "3.0\n",
      "4.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "4.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "Epoch 1/1\n",
      "4176/4176 [==============================] - 17s 4ms/step - loss: 2649512947727.6938 - mean_absolute_error: 876210.5278\n",
      "3.0\n",
      "0.0\n",
      "1.0\n",
      "3.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-a0c0a2216bcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mQ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPlayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mQ\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mQ1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPrepareData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-a0c0a2216bcf>\u001b[0m in \u001b[0;36mPlayGame\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdateQValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-67e5f6df61c3>\u001b[0m in \u001b[0;36mupdateQValues\u001b[1;34m(Q, reward_total)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mstate_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mactions_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN_Agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0maction_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alvar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mc:\\users\\alvar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alvar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alvar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alvar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def PlayGame():\n",
    "    Q = []\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    action = np.random.randint(low=0, high=nb_actions)\n",
    "    img, reward, end_episode, info = env.step(action=action)\n",
    "    state_frame = np.array(preprocess(img))\n",
    "\n",
    "    Q.append([state_frame, action])\n",
    "\n",
    "    action = np.random.randint(low=0, high=nb_actions)\n",
    "    img, reward, end_episode, info = env.step(action=action)\n",
    "    state_frame = np.array(preprocess(img))\n",
    "\n",
    "    Q.append([state_frame, action])\n",
    "\n",
    "    action = np.random.randint(low=0, high=nb_actions)\n",
    "    img, reward, end_episode, info = env.step(action=action)\n",
    "    state_frame = np.array(preprocess(img))\n",
    "\n",
    "    Q.append([state_frame, action])\n",
    "\n",
    "    reward_total = reward\n",
    "    while not (end_episode):\n",
    "\n",
    "        num_states = len(Q)-1\n",
    "        state = np.dstack([Q[num_states-2][0], Q[num_states-1][0], Q[num_states][0]])\n",
    "\n",
    "        state_pred = np.expand_dims(state, axis=0)\n",
    "\n",
    "        if np.random.rand() <= 0.1:\n",
    "            action = np.argmax(DQN_Agent.model.predict(state_pred)[0])\n",
    "        else:\n",
    "            action = np.random.randint(low=0, high=nb_actions)\n",
    "\n",
    "        img, reward, end_episode, info = env.step(action=action)\n",
    "        state_frame = np.array(preprocess(img)) \n",
    "\n",
    "        Q.append([state_frame, action])\n",
    "        print(reward)\n",
    "\n",
    "        reward_total += reward\n",
    "\n",
    "        plt.imshow(state)\n",
    "        plt.title(\"Action Taken : %s\" %env.unwrapped.get_action_meanings()[action])\n",
    "        plt.show()\n",
    "\n",
    "    print(reward_total)\n",
    "\n",
    "    Q = updateQValues(Q,reward_total)\n",
    "    \n",
    "    return Q\n",
    "\n",
    "while(True):\n",
    "    Q = []\n",
    "    for _ in range(15):\n",
    "        Q1 = PlayGame()\n",
    "        Q+=Q1\n",
    "    x,y = PrepareData(Q)\n",
    "    DQN_Agent.model.fit(x=x,y=y,epochs = 1)\n",
    "    del Q\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(Q):\n",
    "    statelist = []\n",
    "    actionslistaaa = []\n",
    "\n",
    "    state0 = Q[0][0]\n",
    "    state = np.dstack([state0, state0, state0])\n",
    "    action0 = Q[2][1]\n",
    "    actionslistaaa.append(np.array(action0))\n",
    "    action0 = Q[2][1]\n",
    "    actionslistaaa.append(np.array(action0))\n",
    "\n",
    "    statelist.append(state)\n",
    "    state0 = Q[1][0]\n",
    "    state = np.dstack([state0, state0, state0])\n",
    "    statelist.append(state)\n",
    "\n",
    "    for i in range (2,len(Q)):\n",
    "        state = np.dstack([Q[i-2][0], Q[i-1][0], Q[i][0]])\n",
    "        statelist.append(state)\n",
    "        action0 = Q[i][1]\n",
    "        actionslistaaa.append(np.array(action0))\n",
    "    x = np.array(statelist)\n",
    "    y = np.array(actionslistaaa)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "413/413 [==============================] - 2s 6ms/step - loss: 0.2631 - mean_absolute_error: 0.2781\n",
      "Epoch 2/5\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 0.2319 - mean_absolute_error: 0.2575\n",
      "Epoch 3/5\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 0.2028 - mean_absolute_error: 0.2516\n",
      "Epoch 4/5\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 0.1900 - mean_absolute_error: 0.2589\n",
      "Epoch 5/5\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 0.1675 - mean_absolute_error: 0.2480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d68e0c8d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN_Agent.model.fit(x=x,y=y,epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Agent = Agent(state_dim,nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_episodes = 1000\n",
    "done = False\n",
    "for e in range(n_episodes): # iterate over new episodes of the game\n",
    "    state = env.reset() # reset state at start of each new episode of the game\n",
    "    state = preprocess(state)\n",
    "    state.resize([1,84, 84,1])\n",
    "    \n",
    "    for time in range(5000):  # time represents a frame of the game; goal is to keep pole upright as long as possible up to range, e.g., 500 or 5000 timesteps\n",
    "        env.render()\n",
    "        action = DQN_Agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
    "        next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position        \n",
    "        reward = reward if not done else -10 # reward +1 for each additional frame with pole upright        \n",
    "        next_state = preprocess(next_state)\n",
    "        next_state.resize([1,84, 84,1])\n",
    "        DQN_Agent.remember(state, action, reward, next_state, done) # remember the previous timestep's state, actions, reward, etc.        \n",
    "        state = next_state # set \"current state\" for upcoming iteration to the current next state        \n",
    "        if done: # episode ends if agent drops pole or we reach timestep 5000\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
    "                  .format(e, n_episodes, time, DQN_Agent.epsilon))\n",
    "            break # exit loop\n",
    "    if len(DQN_Agent.memory) > batch_size:\n",
    "        DQN_Agent.replay(batch_size) # train the agent by replaying the experiences of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Agent.model.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Agent.model = load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
